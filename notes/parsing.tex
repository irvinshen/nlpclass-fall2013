\documentclass[11pt,letterpaper]{article}
\topmargin -.5truein
\textheight 9.0truein
\oddsidemargin 0truein
\evensidemargin 0truein
\textwidth 6.5truein
\setlength{\parskip}{5pt}
\setlength\parindent{0pt}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{caption}
\usepackage{subcaption}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{qtree}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage[all]{xy}
\usetikzlibrary{arrows,snakes,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}


\newcommand{\ra}{\rightarrow}
\newcommand{\bs}{\textbackslash}
%\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\vv}[1]{\ensuremath{\vec{\mathbf{#1}}}}

\newcommand{\ngramstart}{\ensuremath{\langle \textsc{s} \rangle}}
\newcommand{\ngramend}{\ensuremath{\langle \textsc{e} \rangle}}
\newcommand{\ngramunk}{\ensuremath{\langle unk \rangle}}

\newcommand{\wcurr}{\ensuremath{w_i}}
\newcommand{\tcurr}{\ensuremath{t_i}}
\newcommand{\tprev}{\ensuremath{t_{i-1}}}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\title{NLP: Parsing}
\author{Dan Garrette\\\small{dhg@cs.utexas.edu}}

\begin{document}
\maketitle



\section{Grammar}

\vspace{-7mm}\Tree 
  [.S  
    [.NP [.D the ] [.N man ] ] 
    [.VP 
      [.V walks ] 
      [.NP [.D a ] [.N dog ] ] 
    ] 
  ]
\vspace{-2mm}
\begin{itemize}
  \item ``Consitiuency'' parse
  \item S (sentence), NP (noun phrase), VP (verb phrase) are constituents
  \item Words combine to make phrases, and phrases combine to make larger phrases and sentences.
\end{itemize}


\section{Context-Free Grammars (CFG)}

\begin{itemize}
  \item Context-free grammars can be specified by a table of ``productions''
	\begin{center}
	\begin{tabular}{llll p{10mm} lll}
	  S & $\ra$ & NP VP   &                                && D & $\ra$ & \{\textit{the, a}\}      \\
	    &       &         &                                &&   &       &                          \\
	  NP & $\ra$ & D N    & (``\underline{a dog} barks'')  && N & $\ra$ & \{\textit{man, dog}\}    \\
	  NP & $\ra$ & N      & (``\underline{dogs} bark'')    \\
	    &       &         &                                &&   &       &                          \\
	  VP & $\ra$ & V NP   & (transitive verb)              && V & $\ra$ & \{\textit{barks, walks, saw}\} \\
	  VP & $\ra$ & V      & (intransitive verb)
	\end{tabular}
	\end{center}
  \item Words are called ``terminals'' and other nodes are ``non-terminals''
  \item[]
  \item[]
  \item[]
  \item Independence assumption: each rule choice is dependent only on the parent node; it is independent of all other rule choices
  \item Independence assumption leads to many problems.  A couple:
	\begin{figure}[h]
        \begin{subfigure}[b]{0.5\textwidth}
          \begin{small} \Tree [.S [.NP [.D the ] [.N dog ] ] [.VP [.V \textbf{barks} ] [.NP [.D a ] [.N car ] ] ] ]
		  \end{small}
		  \caption{Intransitive verb with a direct object}
        \end{subfigure}
        \begin{subfigure}[b]{0.38\textwidth}
          \begin{small}
                \Tree [.S  [.NP [.D a ] [.N \textbf{dogs} ] ] [.VP [.V barks ] ] ] 
		  \end{small}
		  \caption{Noun doesn't agree in number with determiner or verb}
        \end{subfigure}
	\end{figure}
  \item We can solve some of these issues by refining our CFG.  For example:
	\begin{center}
	\begin{tabular}{llll p{10mm} lll}
	  VP & $\ra$ & IV      & (intransitive verb)            && IV & $\ra$ & \{\textit{barks, walks}\} \\
	  VP & $\ra$ & TV NP   & (transitive verb)              && TV & $\ra$ & \{\textit{chases, walks}\}
	\end{tabular}
	\end{center}
	\begin{figure}[h]
        \begin{subfigure}[b]{0.5\textwidth}
          \begin{small} \Tree [.S [.NP [.D the ] [.N dog ] ] [.VP [.\textbf{TV} \textbf{chases} ] [.NP [.D a ] [.N car ] ] ] ] \end{small}
		  \caption{Intransitive verbs are disallowed}
        \end{subfigure}
        \begin{subfigure}[b]{0.38\textwidth}
          \begin{small} \Tree [.S [.NP [.D the ] [.N dog ] ] [.VP [.\textbf{IV} \textbf{barks} ] ] ] \end{small}
		  \caption{Transitive verbs are disallowed}
        \end{subfigure}
	\end{figure}

\end{itemize}

\newpage
\section{Syntactic Ambiguity}

\begin{itemize}
  \item For a given CFG, there can be multiple trees that describe the same sentence
  \item Add the following rules to the above:
	\begin{center}
	\begin{tabular}{llll p{10mm} lll}
	  NP & $\ra$ & D N PP    & (prep phrase attach to noun)  &&  N & $\ra$ & \{\textit{telescope}\}    \\
	  VP & $\ra$ & V NP PP   & (prep phrase attach to verb)  &&   \\
	  PP & $\ra$ & P NP      & (``in the house'')  &&   
	\end{tabular}
	\end{center}
  \item ``The man saw a dog with a telescope''
	\begin{figure}[h]
		\begin{subfigure}[b]{0.5\textwidth}
		  \begin{small}
		        \Tree 
				  [.S  
				    [.NP [.D the ] [.N man ] ] 
				    [.VP 
				      [.V saw ] 
				      [.NP [.D a ] [.N dog ] ] 
				      [.PP [.P with ] [.NP [.D a ] [.N telescope ] ] ]
				    ] 
				  ]
		  \end{small}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
		  \begin{small}
		        \Tree 
				  [.S  
				    [.NP [.D the ] [.N man ] ] 
				    [.VP 
				      [.V saw ] 
				      [.NP 
				        [.D a ] [.N dog ] 
				        [.PP [.P with ] [.NP [.D a ] [.N telescope ] ] ]
				      ] 
				    ] 
				  ]
		  \end{small}
		\end{subfigure}
	\end{figure}

  \item Ambiguity is explosive
  \item A sentence ending in $n$ prepositional phrases has \textit{over} $2^n$ parses (catalan numbers)
	\begin{itemize}
	  \item ``I saw the man with the telescope'': 2 parses
	  \item ``I saw the man on the hill with the telescope'': 5 parses
	  \item ``I saw the man on the hill in texas with the telescope'': 14 parses
	  \item ``I saw the man on the hill in texas with the telescope at noon'': 42 parses
	  \item ``I saw the mon on the hill in texas with the telescope at noon on monday'': 132 parses
	\end{itemize}
\end{itemize}



\section{Agreement}

\begin{itemize}
  \item In order for a sentence to be grammatical, we must also respect \textit{agreement rules} 
	\begin{itemize}
	  \item number: \textit{a dog} vs. \textit{all dogs}
	  \item person: 1st person (\textit{I am}), 2nd person (\textit{you are}), 3rd person (\textit{he is})
	  \item gender: \textit{un homme} vs. \textit{use femme} or even \textit{she sees herself}
	\end{itemize}

  \item[]
  \item The grammar defined above does not enforce this
	\begin{itemize}
	  \item For an NP, since productions are independent of each other, we could get either \textit{dog} or \textit{dogs} no matter whether the D is \textit{a} or \textit{all}
	\end{itemize}

  \item We can incorporate this into our grammar by duplicating our productions to ensure agreement:
	\begin{center}
	\begin{tabular}{llll p{8mm} lll}
	  S & $\ra$ & NP$_{\text{sg}}$ VP$_{\text{sg}}$                &                                         && D$_{\text{sg}}$ & $\ra$ & \{\textit{the, a}\}      \\
	  S & $\ra$ & NP$_{\text{pl}}$ VP$_{\text{pl}}$                &                                         && D$_{\text{pl}}$ & $\ra$ & \{\textit{the, all}\}    \\
	  \\
	  NP$_{\text{pl}}$ & $\ra$ & N$_{\text{pl}}$                   & (``\underline{dogs} bark'')             && N$_{\text{sg}}$ & $\ra$ & \{\textit{dog}\}    \\
	  NP$_{\text{pl}}$ & $\ra$ & D$_{\text{pl}}$ N$_{\text{pl}}$   & (``\underline{all dogs} bark'')         && N$_{\text{pl}}$ & $\ra$ & \{\textit{dogs}\}    \\
	  NP$_{\text{sg}}$ & $\ra$ & D$_{\text{sg}}$ N$_{\text{sg}}$   & (``\underline{a dog} barks'')    \\
	  \\
	  VP$_{\text{sg}}$ & $\ra$ & V$_{\text{sg}}$                   & (``a dog \underline{barks}'')           && V$_{\text{sg}}$ & $\ra$ & \{\textit{barks, walks, sees}\} \\
	  VP$_{\text{sg}}$ & $\ra$ & V$_{\text{sg}}$ NP$_{\text{sg}}$  & (``a dog \underline{chases a car}'')    && V$_{\text{pl}}$ & $\ra$ & \{\textit{bark, walk, see}\} \\
	  VP$_{\text{sg}}$ & $\ra$ & V$_{\text{sg}}$ NP$_{\text{pl}}$  & (``a dog \underline{chases cars}'')     && \\
	  \\
	  VP$_{\text{pl}}$ & $\ra$ & V$_{\text{pl}}$                   & (``all dogs \underline{bark}'')         && \\
	  VP$_{\text{pl}}$ & $\ra$ & V$_{\text{pl}}$ NP$_{\text{sg}}$  & (``all dogs \underline{chase a car}'')  && \\
	  VP$_{\text{pl}}$ & $\ra$ & V$_{\text{pl}}$ NP$_{\text{pl}}$  & (``all dogs \underline{chase cars}'')   &&
	\end{tabular}
	\end{center}

  \item But this quickly explodes the number of rules
  \item And the explosion is worse with more agreement rules:
	\begin{center}
	\begin{tabular}{llll p{8mm} lll}
	  VP$_{\text{sg,1st,masc}}$ & $\ra$ & V$_{\text{sg,1st,masc}}$            \\
	  VP$_{\text{sg,1st,fem}}$ & $\ra$ & V$_{\text{sg,1st,fem}}$            \\
	  VP$_{\text{sg,2nd,masc}}$ & $\ra$ & V$_{\text{sg,2nd,masc}}$            \\
	  VP$_{\text{sg,2nd,fem}}$ & $\ra$ & V$_{\text{sg,2nd,fem}}$            \\
	  ... \\
	  VP$_{\text{pl,3rd,fem}}$ & $\ra$ & V$_{\text{pl,3rd,fem}}$            \\
	\end{tabular}
	\end{center}

  \item We can simplify this greatly using \textit{feature structures} that use variables to ensure agreement
	\begin{center}
	\begin{tabular}{llll p{8mm} lll}
	  S & $\ra$ & NP$_{\text{[num=$x$]}}$ VP$_{\text{[num=$x$]}}$                         &   && D$_{\text{[num=sg]}}$ & $\ra$ & \{\textit{a, every}\}      \\
	    &       &                                                                         &   && D$_{\text{[num=pl]}}$ & $\ra$ & \{\textit{all, some}\}      \\
	  NP$_{\text{[num=$x$]}}$ & $\ra$ & D$_{\text{[num=$x$]}}$ N$_{\text{[num=$x$]}}$     &   && D$_{\textbf{[num=$z$]}}$ & $\ra$ & \{\textit{the}\}      \\
	    &       &                                                                         &   \\
	  VP$_{\text{[num=$x$]}}$ & $\ra$ & V$_{\text{[num=$x$]}}$                            &   && N$_{\text{[num=sg]}}$ & $\ra$ & \{\textit{dog}\}      \\
	  VP$_{\text{[num=$x$]}}$ & $\ra$ & V$_{\text{[num=$x$]}}$ NP$_{\textbf{[num=$y$]}}$  &   && N$_{\text{[num=pl]}}$ & $\ra$ & \{\textit{dogs}\}      \\
	  \\
	    &       &                                                                         &   && V$_{\text{[num=sg]}}$ & $\ra$ & \{\textit{barks, walks, sees}\} \\
	    &       &                                                                         &   && V$_{\text{[num=pl]}}$ & $\ra$ & \{\textit{bark, walk, see}\} \\
	\end{tabular}
	\end{center}

  \item[]
  \item[]
  \item[]
  \item Feature structures must \textit{unify} as they are combined
	\begin{itemize}
	  \item For example, all $x$s must resolve to the same value (\textit{sg} or \textit{pl})
	\end{itemize}

	\begin{figure}[h]
	    \centering
		\begin{subfigure}[t]{0.4\textwidth}
		  \begin{small}
			\Tree 
			  [.S  
			    [.NP$_{\text{[num=$x$]}}$ 
			      [.D$_{\text{[num=$x$]}}$\\\\\\D$_{\text{[num=$z$]}}$ the ] 
			      [.N$_{\text{[num=$x$]}}$\\\\\\N$_{\text{[num=sg]}}$ dog ] ] 
			    [.VP$_{\text{[num=$x$]}}$  
			      [.V$_{\text{[num=$x$]}}$\\\\\\V$_{\text{[num=sg]}}$ barks ] ] 
			  ]
		  \end{small}
		  \caption{Need to unify underspecified features in the rules with secified features in the N and V terminal rules}
		\end{subfigure}
		~~~~~~  ~~~~~
		\begin{subfigure}[t]{0.3\textwidth}
		  \begin{small}
			\Tree 
			  [.S  
			    [.NP$_{\text{[num=sg]}}$ [.D$_{\text{[num=sg]}}$ the ] [.N$_{\text{[num=sg]}}$ dog ] ] 
			    [.VP$_{\text{[num=sg]}}$ [.V$_{\text{[num=sg]}}$ barks ] ] 
			  ]
		  \end{small}
		  \caption{all $x$s resolve to ``sg''}
		\end{subfigure}
	\end{figure}

  \item Similar features could be set up for other required agreements
	\begin{itemize}
	  \item PRO[num=sg, per=3rd, gen=masc] $\rightarrow$ \textit{il}
	\end{itemize}

\end{itemize}



\section{Probabilistic Context-Free Grammars (PCFG)}

\begin{itemize}
  \item A CFG is deterministic
	\begin{itemize}
	  \item It can decide whether a sentence is in the language (grammatical), or not
	  \item It can't judge whether one sentence is more likely than another
	  \item Problematic since we want to say that every sentence is \textit{possible}, even if it's not likely
	\end{itemize}
  \item A PCFG is a CFG in which, for every non-terminal, we have a probability distribution over possible productions
  \item In other words, for each non-terminal A, we have a distribution $p(\beta \mid A)$
	\begin{itemize}
	  \item The probability that, given a parent non-terminal A, we choose the production rule that yields $\beta$
	  \item We can alternatively write this as $p(A \ra \beta)$
	\end{itemize}
\end{itemize}

	\begin{center}
	\begin{tabular}{llll p{18mm} llll}
	  S  & $\ra$ & NP VP    &  1.0  &&  D & $\ra$ & \textit{the}        & 0.6 \\
	     &       &          &       &&  D & $\ra$ & \textit{a}          & 0.4 \\
	  NP & $\ra$ & D N      &  0.7  &&    &       &                     &     \\
	  NP & $\ra$ & N        &  0.2  &&  N & $\ra$ & \textit{man}        & 0.5 \\
	  NP & $\ra$ & D N PP   &  0.1  &&  N & $\ra$ & \textit{dog}        & 0.4 \\
	     &       &          &       &&  N & $\ra$ & \textit{telescope}  & 0.1 \\
	  VP & $\ra$ & V NP     &  0.4  &&    &       &                     &     \\
	  VP & $\ra$ & V        &  0.4  &&  V & $\ra$ & \textit{barks}      & 0.2 \\
	  VP & $\ra$ & V NP PP  &  0.2  &&  V & $\ra$ & \textit{walks}      & 0.4 \\
	     &       &          &       &&  V & $\ra$ & \textit{saw}        & 0.4
	\end{tabular}
	\end{center}


Estimating parameters (MLE)

\begin{itemize}
  \item We can calculate the MLE parameters of a PCFG model by counting productions in a corpus of parse trees
  \item Assume these three sentences comprise a corpus:
	\begin{figure}[h]
	        \centering
	        \begin{subfigure}[b]{0.3\textwidth}
	          \begin{small} \Tree [.S [.NP [.D the ] [.N dog ] ] [.VP [.V ran ] ] ] \end{small}
	        \end{subfigure}
	        \begin{subfigure}[b]{0.3\textwidth}
	          \begin{small} \Tree [.S  [.NP [.D the ] [.N man ] ] [.VP [.V walked ] [.NP [.D a ] [.N dog ] ] ] ] \end{small}
	        \end{subfigure}
	        \begin{subfigure}[b]{0.3\textwidth}
	          \begin{small} \Tree [.S  [.NP [.D the ] [.N man ] ] [.VP [.V walked ] ] ] \end{small}
	        \end{subfigure}
	\end{figure}
  \item We estimate by counting up all productions in the corpus and normalizing
    \begin{itemize}
      \item Since every non-terminal $A$ must produce \textit{something}, we can simplify slightly
    \end{itemize}
    \[ p(\beta \mid A) = \frac{C(A \ra\beta)}{\sum_{\beta' \in P} C(A \ra\beta')} = \frac{C(A \ra\beta)}{C(A)} \]
  \item Estimates from the above corpus of trees yield: \vspace{-3mm}
	\begin{center}
	\begin{tabular}{lllcc p{13mm} lllcc}
	     &       &          & $C(A\ra\beta)$ & $p(\beta\mid A)$ &&    &       &                   & $C(A\ra\beta)$ & $p(\beta\mid A)$ \\
	  \hline
	  S  & $\ra$ & NP VP    &  3             & 1.0              &&  D & $\ra$ & \textit{the}      & 3              & 0.75             \\
	     &       &          &                &                  &&  D & $\ra$ & \textit{a}        & 1              & 0.25             \\
	  NP & $\ra$ & D N      &  4             & 1.0              &&    &       &                   &                &                  \\
	     &       &          &                &                  &&  N & $\ra$ & \textit{dog}      & 2              & 0.5              \\
	  VP & $\ra$ & V        &  2             & 0.66             &&  N & $\ra$ & \textit{man}      & 2              & 0.5              \\
	  VP & $\ra$ & V NP     &  1             & 0.33             \\
	     &       &          &                &                  &&  V & $\ra$ & \textit{walked}   & 2              & 0.66             \\
	     &       &          &                &                  &&  V & $\ra$ & \textit{ran}      & 1              & 0.33             
	\end{tabular}
	\end{center}
\end{itemize}


Add-$\lambda$ smoothing

\begin{itemize}
  \item Same as always (where $P$ is the set of all possible $\beta$s produced):
    \[ p(\beta \mid A) 
      = \frac{C(A \ra\beta)+\lambda}{\sum_{\beta' \in P} (C(A \ra\beta')+\lambda)} 
      = \frac{C(A \ra\beta)+\lambda}{(\sum_{\beta' \in P} C(A \ra\beta'))+\lambda|P|} 
      = \frac{C(A \ra\beta)+\lambda}{C(A)+\lambda|P|} \]
\end{itemize}


Likelihood of a (parsed) sentence

\begin{itemize}
  \item We can use this to calculate the likelihood of seeing a particular parse of a particular sentence
  \item Multiply the probabilities of all productions found in the tree
  \item Assume this tree:
	\begin{small} \Tree [.S [.NP [.D the ] [.N dog ] ] [.VP [.V walked ] [.NP [.D a ] [.N man ] ] ] ] \end{small}
  \item Count up the number of each production in the tree, and get the probability of each production from the above table
	\begin{center}
	\begin{tabular}{lllcc p{13mm} lllcc}
	     &       &          & count & prob  &&    &       &                   & count & prob \\
	  \hline
	  S  & $\ra$ & NP VP    &  1    & 1.0   &&  D & $\ra$ & \textit{the}      & 1     & 0.75             \\
	     &       &          &       &       &&  D & $\ra$ & \textit{a}        & 1     & 0.25             \\
	  NP & $\ra$ & D N      &  2    & 1.0   &&    &       &                   &       &                  \\
	     &       &          &       &       &&  N & $\ra$ & \textit{dog}      & 1     & 0.5              \\
	  VP & $\ra$ & V NP     &  1    & 0.33  &&  N & $\ra$ & \textit{man}      & 1     & 0.5              \\
	     &       &          &       &       \\
	     &       &          &       &       &&  V & $\ra$ & \textit{walked}   & 1     & 0.66             \\
	\end{tabular}
	\end{center}
  \item Find the product:
    \begin{align*}
      & p(\text{S}\ra\text{NP VP}) \cdot
        p(\text{NP}\ra\text{D N})^{2} \cdot
        p(\text{VP}\ra\text{V NP}) \cdot
        p(\text{D}\ra\textit{the}) \cdot
        p(\text{D}\ra\textit{a}) \cdot
        p(\text{N}\ra\textit{dog}) \\&~~~\cdot
        p(\text{N}\ra\textit{man}) \cdot
        p(\text{V}\ra\textit{walked}) \\
      &= 1.0 \cdot 1.0^{2} \cdot 0.33 \cdot 0.75 \cdot 0.25 \cdot 0.5 \cdot 0.5 \cdot 0.66
    \end{align*}
	\item Note that the $p(\text{NP}\ra\text{D N})$ term is squared since the production ``$\text{NP}\ra\text{D N}$'' appears twice in the tree
\end{itemize}




\section{Lexicalized Grammars}

\begin{itemize}
  \item Problem: trees do not take semantic coherence into account
  \item Production rules are independent
  \item These two sentence have \textit{exactly} the same likelihood
	\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
		  \begin{small} \Tree [.S [.NP [.D the ] [.N \textbf{dog} ] ] [.VP [.V bit ] [.NP [.D a ] [.N \textbf{man} ] ] ] ] \end{small}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
		  \begin{small} \Tree [.S [.NP [.D the ] [.N \textbf{man} ] ] [.VP [.V bit ] [.NP [.D a ] [.N \textbf{dog} ] ] ] ] \end{small}
        \end{subfigure}
	\end{figure}
  \item Since it's a product, we can swap two productions without changing the result
	\begin{small}
    \[
       p(\text{S}\ra\text{NP VP}) \cdot
        p(\text{NP}\ra\text{D N})^{2} \cdot
        p(\text{VP}\ra\text{V NP}) \cdot
        p(\text{D}\ra\textit{the}) \cdot
        p(\text{D}\ra\textit{a}) \cdot
        \mathbf{p(\textbf{N}\ra\textbf{\textit{dog}})} \cdot
        \mathbf{p(\textbf{N}\ra\textbf{\textit{man}})} \cdot
        p(\text{V}\ra\textit{bit}) 
        \vspace{-2.5mm}\]
        \hspace{4.5in}$\mathbf{\downarrow}$\hspace{0.8in}$\mathbf{\downarrow}$
        \vspace{-2mm}
    \[ p(\text{S}\ra\text{NP VP}) \cdot
        p(\text{NP}\ra\text{D N})^{2} \cdot
        p(\text{VP}\ra\text{V NP}) \cdot
        p(\text{D}\ra\textit{the}) \cdot
        p(\text{D}\ra\textit{a}) \cdot
        \mathbf{p(\textbf{N}\ra\textbf{\textit{man}})} \cdot
        \mathbf{p(\textbf{N}\ra\textbf{\textit{dog}})} \cdot
        p(\text{V}\ra\textit{bit}) \]
    \end{small}
  \item Solution: lexicalize the grammar
  \item Subcategorize rules with their head words:
	\begin{center}
	\begin{tabular}{llll p{18mm} llll}
	  S  & $\ra$ & NP(man) VP(bite)            &  0.3  &&  D & $\ra$ & \textit{the}        & 0.6 \\
	  S  & $\ra$ & NP(dog) VP(bite)            &  0.7  &&  D & $\ra$ & \textit{a}          & 0.4 \\
	  S$^1$  & $\ra$ & NP(sandwich) VP(bite)       &  0.0  &&    &       &                     &     \\
	     &       &                             &       &&    &       &                     &     \\
	  NP(man)$^2$ & $\ra$ & D N(man)               &  0.8  &&  N(man)$^2$ & $\ra$ & \textit{man}   & 0.7 \\
	  NP(man)$^2$ & $\ra$ & N(man)                 &  0.2  &&  N(man)$^2$ & $\ra$ & \textit{men}   & 0.3 \\
	     &       &                             &       &&    &       &                     &     \\
	  NP(dog)$^3$ & $\ra$ & D N(dog)               &  0.4  &&  N(dog)$^3$ & $\ra$ & \textit{dog}   & 0.2 \\
	  NP(dog)$^3$ & $\ra$ & N(dog)                 &  0.6  &&  N(dog)$^3$ & $\ra$ & \textit{dogs}  & 0.8 \\
	     &       &                             &       &&    &       &                     &     \\
	  VP(bite)$^4$ & $\ra$ & V(bite) NP(man)       &  0.2  &&  V(bite) & $\ra$ & \textit{bite}    & 0.3 \\
	  VP(bite)$^4$ & $\ra$ & V(bite) NP(dog)       &  0.1  &&  V(bite) & $\ra$ & \textit{bites}   & 0.2 \\
	  VP(bite)$^4$ & $\ra$ & V(bite) NP(sandwich)  &  0.7  &&  V(bite) & $\ra$ & \textit{biting}  & 0.1 \\
	  VP(bite)$^5$ & $\ra$ & V(bite)               &  0.0  &&  V(bite) & $\ra$ & \textit{bit}     & 0.4 \\
	\end{tabular}
	\end{center}
  \begin{itemize}
    \item {}$^1$Sandwiches don't bite
    \item {}$^2$Men are likely talked about in the singular
    \item {}$^3$Dogs are likely talked about in the plural
    \item {}$^4$Men are bitten infrequently, dogs are bitten less frequently, and sandwiches are bitten often.
    \item {}$^5$``Biting'' can't be intransitive
  \end{itemize}

	\begin{figure}[h]
	        \centering
	        \begin{subfigure}[b]{0.5\textwidth}
	          \begin{small}
	                \Tree 
					  [.S  
					    [.NP(dog) [.D the ] [.N(dog) \textbf{dog} ] ] 
					    [.VP(bite) 
					      [.V(bite) bit ] 
					      [.NP(man) [.D a ] [.N(man) \textbf{man} ] ] 
					    ] 
					  ]
			  \end{small}
			  \caption{more likely}
	        \end{subfigure}
	        \begin{subfigure}[b]{0.3\textwidth}
	          \begin{small}
	                \Tree 
					  [.S  
					    [.NP(man) [.D the ] [.N(man) \textbf{man} ] ] 
					    [.VP(bite) 
					      [.V(bite) bit ] 
					      [.NP(dog) 
					        [.D a ] [.N(dog) \textbf{dog} ] 
					      ] 
					    ] 
					  ]
			  \end{small}
			  \caption{unlikely}
	        \end{subfigure}
	\end{figure}

  \item Downside: increased sparsity!
\end{itemize}


\newpage
\section{Generative Model}

\begin{itemize}
  \item Like na\"{i}ve Bayes models, N-Gram models, and hidden Markov models
  \item Two probability distribution: $p(\beta \mid \alpha)$, for production rules $\alpha \ra \beta$, and $p(\sigma)$, where $\sigma$ is a possible ``start'' symbol
  \item Generative story:
    \begin{enumerate}
      \item Choose a start symbol $x$ from the distribution over start symbols $p(\sigma)$
      \item If $x$ is a terminal, STOP
      \item Else, choose some $\beta$ from $p(\beta \mid x)$
      \item ~~~For each symbol $y$ in $\beta$, go to step 2
    \end{enumerate}
  \item For each node with symbol $x$, we choose a production rule of the form $x \ra \beta$ according to their probabilities and then recursively choose rules for every node in $\beta$ until we reach terminals for all branches.
\end{itemize}


\section{Parsing with P-CKY}

Find the mostly likely parse tree $t$ for the sentence $w_0...w_n$
\begin{itemize}
  \item $ \hat{t} = \text{argmax}_t~p(t \mid w_0...w_n) $
  \item Since we know how to find the likelihood of a parse (from above), we could enumerate all possible trees, and find the likelihood of each one.
  \item But the number of possible trees is enormous
  \item We will use a \textit{dynamic programming} algorithm to find the best parse tree: P-CKY
  \item Probabilistic version of the CKY algorithm, which can be used with any CFG
  \item Analgous to the Viterbi algorithm (both are dynamic programming algorithms)
\end{itemize}


\section{N-Best Parses}

Useful for providing multiple possiblities to down-stream applications.

Can be fed into a discriminative re-ranker that uses something like MaxEnt with features built from linguistic knowlege to re-score the parses.  The discriminative model isn't use for \textit{parsing}, just to calculate the likelihoods of the parses that were found.




\section{Other Grammatical Formalisms}

TAG: Tree-Adjoining Grammar: http://www.seas.upenn.edu/$\sim$joshi/joshi-schabes-tag-97.pdf

CCG: Combinatory Categorial Grammar

Dependency Parsing





\end{document}

